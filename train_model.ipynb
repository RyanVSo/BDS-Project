{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54401260-0a9e-4aca-a265-7b08d1613001",
   "metadata": {},
   "source": [
    "## Imports and loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca516b6b-fd0b-41e8-9f71-4ca452d18689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard libraries\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Importing sklearn libraries\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, log_loss, roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, Normalizer, StandardScaler\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# Importing nltk libraries\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Importing scipy library\n",
    "from scipy.sparse import hstack\n",
    "\n",
    "# Importing xgboost library\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Suppressing warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Installing xgboost\n",
    "!pip install xgboost\n",
    "\n",
    "# Loading the data\n",
    "path = '../BDS_project/final_new_data_processed.csv'\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "# Preparing the data\n",
    "X = data.drop(['review','rating','date','review_sentiment'],axis=1)\n",
    "y = data['review_sentiment'].values\n",
    "\n",
    "# Splitting the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.30, random_state=42)\n",
    "X_train, X_cv, y_train, y_cv = train_test_split(X_train, y_train, stratify=y_train, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ea2f0-270b-4dbf-8365-d9a2738f2c5a",
   "metadata": {},
   "source": [
    "## Initialization of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e5e09b-6e63-4a71-ace3-322dc26bb36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Define the columns to be normalized\n",
    "columns = ['usefulCount', 'sentiment_score', 'sentiment_score_clean', 'word_count',\n",
    "       'unique_word_count', 'char_length', 'count_punctuations',\n",
    "       'stopword_count', 'mean_word_len', 'subj_count', 'obj_count',\n",
    "       'CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC',\n",
    "       'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT',\n",
    "       'QUANTITY', 'TIME', 'WORK_OF_ART', '0', '1', '2', '3', '4', '5', '6',\n",
    "       '7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18',\n",
    "       '19']\n",
    "\n",
    "# Normalize the specified columns of the training, testing, and cross-validation data\n",
    "X_train_num_1 = normalizer.fit_transform(X_train[columns])\n",
    "X_test_num_1 = normalizer.transform(X_test[columns])  # Use transform, not fit_transform on test data\n",
    "X_cv_num_1 = normalizer.transform(X_cv[columns])  # Use transform, not fit_transform on CV data\n",
    "\n",
    "# Extract the 'sentiment_score' and 'sentiment_score_clean' columns from the datasets\n",
    "X_train_sent_score = X_train[['sentiment_score', 'sentiment_score_clean']].values\n",
    "X_test_sent_score = X_test[['sentiment_score', 'sentiment_score_clean']].values\n",
    "X_cv_sent_score = X_cv[['sentiment_score', 'sentiment_score_clean']].values\n",
    "\n",
    "# Concatenate the normalized data and the sentiment scores for each dataset\n",
    "X_tr_1 = np.concatenate((X_train_num_1, X_train_sent_score), axis=1)\n",
    "X_te_1 = np.concatenate((X_test_num_1, X_test_sent_score), axis=1)\n",
    "X_cv_1 = np.concatenate((X_cv_num_1, X_cv_sent_score), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59bebeb-8aa8-42c5-b83d-95747c55b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder\n",
    "lab_enc_year = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the 'year' column of the full dataset\n",
    "lab_enc_year.fit(X['year'].values)\n",
    "\n",
    "# Transform the 'year' column of the training, testing, and cross-validation data\n",
    "# The reshape(-1, 1) is used to ensure the output is a 2D array\n",
    "X_train_year = lab_enc_year.transform(X_train['year'].values).reshape(-1, 1)\n",
    "X_test_year = lab_enc_year.transform(X_test['year'].values).reshape(-1, 1)\n",
    "X_cv_year = lab_enc_year.transform(X_cv['year'].values).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954fb4d1-a449-41ce-9698-b84f5e01b55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to be dropped\n",
    "drop_col = ['subj_count', 'obj_count', 'CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY',\n",
    " 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', '9', '14']\n",
    "\n",
    "# Drop the specified columns from the training, testing, and cross-validation data\n",
    "X_train = X_train.drop(drop_col, axis=1)\n",
    "X_test = X_test.drop(drop_col, axis=1)\n",
    "X_cv = X_cv.drop(drop_col, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43f881a-269a-4a1d-885a-ee105f35142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the important columns to be normalized\n",
    "imp_columns = ['usefulCount', 'word_count', 'unique_word_count', 'char_length', 'count_punctuations',\n",
    "               'stopword_count', 'mean_word_len', 'TIME', 'WORK_OF_ART', '0', '1', '2',\n",
    "               '3', '4', '5', '6', '7', '8', '10', '11', '12', '13', '15', '16', '17', '18', '19']\n",
    "\n",
    "# Initialize the Normalizer\n",
    "normalizer = Normalizer()\n",
    "\n",
    "# Normalize the specified columns of the training, testing, and cross-validation data\n",
    "X_train_num_2 = normalizer.fit_transform(X_train[imp_columns])\n",
    "X_test_num_2 = normalizer.transform(X_test[imp_columns])  # Use transform, not fit_transform on test data\n",
    "X_cv_num_2 = normalizer.transform(X_cv[imp_columns])  # Use transform, not fit_transform on CV data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7535ac-693c-4000-99c8-d5fc7f39c05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder\n",
    "lab_enc_cond = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the 'condition' column of the full dataset\n",
    "lab_enc_cond.fit(X['condition'].values)\n",
    "\n",
    "# Transform the 'condition' column of the training, testing, and cross-validation data\n",
    "# The reshape(-1, 1) is used to ensure the output is a 2D array\n",
    "X_train_condition = lab_enc_cond.transform(X_train['condition'].values).reshape(-1, 1)\n",
    "X_test_condition = lab_enc_cond.transform(X_test['condition'].values).reshape(-1, 1)\n",
    "X_cv_condition = lab_enc_cond.transform(X_cv['condition'].values).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5e1c7-4e05-4ec0-a87a-7cb61cc4d569",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW) Vectorization + XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34cc3c2c-65d8-451a-8dae-dcbbd1f145e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved vectorizer\n",
    "vect_bow_1 = load('../BDS_project/vectorizer_bow.pkl')\n",
    "\n",
    "# Transform the 'cleaned_review' column of the training, testing, and cross-validation data\n",
    "X_train_review_bow_1 = vect_bow_1.transform(X_train['cleaned_review'].values)\n",
    "X_test_review_bow_1 = vect_bow_1.transform(X_test['cleaned_review'].values)\n",
    "X_cv_review_bow_1 = vect_bow_1.transform(X_cv['cleaned_review'].values)\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "lab_enc_cond = LabelEncoder()\n",
    "\n",
    "# Fit the encoder on the 'condition' column of the full dataset\n",
    "lab_enc_cond.fit(X['condition'].values)\n",
    "\n",
    "# Transform the 'condition' column of the training, testing, and cross-validation data\n",
    "# The reshape(-1, 1) is used to ensure the output is a 2D array\n",
    "X_train_condition = lab_enc_cond.transform(X_train['condition'].values).reshape(-1, 1)\n",
    "X_test_condition = lab_enc_cond.transform(X_test['condition'].values).reshape(-1, 1)\n",
    "X_cv_condition = lab_enc_cond.transform(X_cv['condition'].values).reshape(-1, 1)\n",
    "\n",
    "# Concatenate the normalized data, sentiment scores, condition, year, and bag-of-words features for each dataset\n",
    "X_tr_2 = hstack((X_train_num_2, X_train_sent_score, X_train_condition, X_train_year, X_train_review_bow_1)).tocsr()\n",
    "X_te_2 = hstack((X_test_num_2, X_test_sent_score, X_test_condition, X_test_year, X_test_review_bow_1)).tocsr()\n",
    "X_cv_2 = hstack((X_cv_num_2, X_cv_sent_score, X_cv_condition, X_cv_year, X_cv_review_bow_1)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ee9d9a-1404-4fb1-9ba9-6f92aad1d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation set for early stopping\n",
    "eval_set = [(X_tr_2, y_train), (X_cv_2, y_cv)]\n",
    "\n",
    "# Initialize the XGBoost classifier with specified hyperparameters\n",
    "x_cfl_2 = XGBClassifier(n_estimators=1000, subsample=1, max_depth=10, learning_rate=0.1, colsample_bytree=0.1, nthread=-1, objective='binary:logistic', random_state=0)\n",
    "\n",
    "# Fit the classifier on the training data and use early stopping based on logloss on the evaluation set\n",
    "x_cfl_2.fit(X_tr_2, y_train, eval_set=eval_set, eval_metric='logloss', verbose=0, early_stopping_rounds=20)\n",
    "\n",
    "# Initialize a calibrated classifier on the XGBoost classifier with sigmoid method\n",
    "x_sig_clf_2 = CalibratedClassifierCV(x_cfl_2, method=\"sigmoid\")\n",
    "\n",
    "# Fit the calibrated classifier on the training data\n",
    "x_sig_clf_2.fit(X_tr_2, y_train)\n",
    "\n",
    "# Call the function to calculate and display the model metrics\n",
    "model_metrics(x_sig_clf_2, X_tr_2, X_te_2, X_cv_2)\n",
    "\n",
    "# Save the trained model for future use\n",
    "joblib.dump(x_sig_clf_2, '../BDS_project/Bow_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0bc772-9618-48ed-9896-8e26ef020fce",
   "metadata": {},
   "source": [
    "## Term Frequency - Inverse Document Frequency (TF-IDF) Vectorization + XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ef8a59-df71-4881-9f20-d8ea019a94ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved vectorizer\n",
    "vect_tfidf_1 = joblib.load('../BDS_project/vectorizer_tfidf.pkl')\n",
    "\n",
    "# Transform the 'cleaned_review' column of the training, testing, and cross-validation data\n",
    "X_train_review_tfidf_1 = vect_tfidf_1.transform(X_train['cleaned_review'].values)\n",
    "X_test_review_tfidf_1 = vect_tfidf_1.transform(X_test['cleaned_review'].values)\n",
    "X_cv_review_tfidf_1 = vect_tfidf_1.transform(X_cv['cleaned_review'].values)\n",
    "\n",
    "# Concatenate the normalized data, sentiment scores, condition, year, and TF-IDF features for each dataset\n",
    "X_tr_3 = hstack((X_train_num_2, X_train_sent_score, X_train_condition, X_train_year, X_train_review_tfidf_1)).tocsr()\n",
    "X_te_3 = hstack((X_test_num_2, X_test_sent_score, X_test_condition, X_test_year, X_test_review_tfidf_1)).tocsr()\n",
    "X_cv_3 = hstack((X_cv_num_2, X_cv_sent_score, X_cv_condition, X_cv_year, X_cv_review_tfidf_1)).tocsr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66809002-e98f-4398-9788-263d3e27de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation set for early stopping\n",
    "eval_set = [(X_tr_3, y_train), (X_cv_3, y_cv)]\n",
    "\n",
    "# Initialize the XGBoost classifier with specified hyperparameters\n",
    "x_cfl_3 = XGBClassifier(n_estimators=3000, subsample=0.3, max_depth=7, learning_rate=0.05, colsample_bytree=1, nthread=-1, objective='binary:logistic', random_state=0)\n",
    "\n",
    "# Fit the classifier on the training data and use early stopping based on logloss on the evaluation set\n",
    "x_cfl_3.fit(X_tr_3, y_train, eval_set=eval_set, eval_metric='logloss', verbose=0, early_stopping_rounds=30)\n",
    "\n",
    "# Initialize a calibrated classifier on the XGBoost classifier with sigmoid method\n",
    "x_sig_clf_3 = CalibratedClassifierCV(x_cfl_3, method=\"sigmoid\")\n",
    "\n",
    "# Fit the calibrated classifier on the training data\n",
    "x_sig_clf_3.fit(X_tr_3, y_train)\n",
    "\n",
    "# Call the function to calculate and display the model metrics\n",
    "model_metrics(x_sig_clf_3, X_tr_3, X_te_3, X_cv_3)\n",
    "\n",
    "# Save the trained model for future use\n",
    "joblib.dump(x_sig_clf_3, '../BDS_project/tfidf_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e418742-1957-4c39-810e-c549a0c386b8",
   "metadata": {},
   "source": [
    "## N-Gram + BoW Vectorization + XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f055575d-556e-44e7-9aae-b0854332a019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved vectorizer\n",
    "vec_bow = joblib.load('../BDS_project/ngram_vec_bow.pkl')\n",
    "\n",
    "# Transform the 'cleaned_review' column of the training, testing, and cross-validation data\n",
    "X_train_review_bow_ngram = vec_bow.transform(X_train['cleaned_review'].values)\n",
    "X_test_review_bow_ngram = vec_bow.transform(X_test['cleaned_review'].values)\n",
    "X_cv_review_bow_ngram = vec_bow.transform(X_cv['cleaned_review'].values)\n",
    "\n",
    "# Concatenate the normalized data, sentiment scores, condition, year, and bag-of-words features for each dataset\n",
    "X_tr_4 = hstack((X_train_num_2, X_train_sent_score, X_train_condition, X_train_year, X_train_review_bow_ngram)).tocsr()\n",
    "X_te_4 = hstack((X_test_num_2, X_test_sent_score, X_test_condition, X_test_year, X_test_review_bow_ngram)).tocsr()\n",
    "X_cv_4 = hstack((X_cv_num_2, X_cv_sent_score, X_cv_condition, X_cv_year, X_cv_review_bow_ngram)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edddd1cc-22da-4241-850d-3d04cd9b0534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation set for early stopping\n",
    "eval_set = [(X_tr_4, y_train), (X_cv_4, y_cv)]\n",
    "\n",
    "# Initialize the XGBoost classifier with specified hyperparameters\n",
    "x_cfl_4 = XGBClassifier(n_estimators=3000, subsample=0.5, max_depth=10, learning_rate=0.03, colsample_bytree=0.3, nthread=-1, objective='binary:logistic', random_state=0)\n",
    "\n",
    "# Fit the classifier on the training data and use early stopping based on logloss on the evaluation set\n",
    "x_cfl_4.fit(X_tr_4, y_train, eval_set=eval_set, eval_metric='logloss', verbose=0, early_stopping_rounds=30)\n",
    "\n",
    "# Initialize a calibrated classifier on the XGBoost classifier with sigmoid method\n",
    "x_sig_clf_4 = CalibratedClassifierCV(x_cfl_4, method=\"sigmoid\")\n",
    "\n",
    "# Fit the calibrated classifier on the training data\n",
    "x_sig_clf_4.fit(X_tr_4, y_train)\n",
    "\n",
    "# Call the function to calculate and display the model metrics\n",
    "model_metrics(x_sig_clf_4, X_tr_4, X_te_4, X_cv_4)\n",
    "\n",
    "# Save the trained model for future use\n",
    "joblib.dump(x_sig_clf_4, '../BDS_project/ngram_bow_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1c3cd-c238-4769-8026-fb2a5a53baed",
   "metadata": {},
   "source": [
    "## N-Gram + TF-IDF + XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecd1a51-40be-4926-9fbe-588e50fd6240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the previously saved vectorizer\n",
    "vec_tfidf = joblib.load('../BDS_project/ngram_vec_tfidf.pkl')\n",
    "\n",
    "# Transform the 'cleaned_review' column of the training, testing, and cross-validation data\n",
    "X_train_review_tfidf_ngram = vec_tfidf.transform(X_train['cleaned_review'].values)\n",
    "X_test_review_tfidf_ngram = vec_tfidf.transform(X_test['cleaned_review'].values)\n",
    "X_cv_review_tfidf_ngram = vec_tfidf.transform(X_cv['cleaned_review'].values)\n",
    "\n",
    "# Concatenate the normalized data, sentiment scores, condition, year, and TF-IDF features for each dataset\n",
    "X_tr_5 = hstack((X_train_num_2, X_train_sent_score, X_train_condition, X_train_year, X_train_review_tfidf_ngram)).tocsr()\n",
    "X_te_5 = hstack((X_test_num_2, X_test_sent_score, X_test_condition, X_test_year, X_test_review_tfidf_ngram)).tocsr()\n",
    "X_cv_5 = hstack((X_cv_num_2, X_cv_sent_score, X_cv_condition, X_cv_year, X_cv_review_tfidf_ngram)).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc895acb-c853-458d-99a0-f8c1ff2fb6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation set for early stopping\n",
    "eval_set = [(X_tr_5, y_train), (X_cv_5, y_cv)]\n",
    "\n",
    "# Initialize the XGBoost classifier with specified hyperparameters\n",
    "x_cfl_5 = XGBClassifier(n_estimators=3000, subsample=0.5, max_depth=10, learning_rate=0.2, colsample_bytree=0.1, nthread=-1, objective='binary:logistic', random_state=0)\n",
    "\n",
    "# Fit the classifier on the training data and use early stopping based on logloss on the evaluation set\n",
    "x_cfl_5.fit(X_tr_5, y_train, eval_set=eval_set, eval_metric='logloss', verbose=0, early_stopping_rounds=30)\n",
    "\n",
    "# Initialize a calibrated classifier on the XGBoost classifier with sigmoid method\n",
    "x_sig_clf_5 = CalibratedClassifierCV(x_cfl_5, method=\"sigmoid\")\n",
    "\n",
    "# Fit the calibrated classifier on the training data\n",
    "x_sig_clf_5.fit(X_tr_5, y_train)\n",
    "\n",
    "# Call the function to calculate and display the model metrics\n",
    "model_metrics(x_sig_clf_5, X_tr_5, X_te_5, X_cv_5)\n",
    "\n",
    "# Save the trained model for future use\n",
    "joblib.dump(x_sig_clf_5, '../BDS_project/ngram_tfidf_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ed2094-94b7-4b1b-88f0-e303a3ec3e55",
   "metadata": {},
   "source": [
    "## Word2Vec + XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67525f01-21f6-47f7-a6f1-a0b50aba09c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Initialize an empty list to store the sentences\n",
    "sentences = []\n",
    "\n",
    "# Loop through each review in the 'cleaned_review' column of the data\n",
    "# Split each review into words and append to the sentences list\n",
    "for r in tqdm(data['cleaned_review']):\n",
    "    sentences.append(r.split())\n",
    "\n",
    "# Train a Word2Vec model on the sentences\n",
    "model = Word2Vec(sentences, vector_size=300, workers=12, min_count=1)\n",
    "\n",
    "# Print the trained model\n",
    "print(model)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model.save('word2vec.bin')\n",
    "\n",
    "# Load the trained model from the file\n",
    "model = Word2Vec.load('word2vec.bin')\n",
    "\n",
    "# Define a function to create word2vec vectors for the reviews\n",
    "def create_w2v(df, feature):\n",
    "    w2v_vector = []\n",
    "    for review in df[feature]:\n",
    "        vector = np.zeros(300)\n",
    "        for word in review.split():\n",
    "            if word in model.wv.key_to_index:\n",
    "                vector += model.wv[word]\n",
    "        w2v_vector.append(vector)\n",
    "    w2v_vector = np.array(w2v_vector)\n",
    "    return w2v_vector\n",
    "\n",
    "# Create word2vec vectors for the 'cleaned_review' column of the training, testing, and cross-validation data\n",
    "X_train_review_w2v = create_w2v(X_train, 'cleaned_review')\n",
    "X_test_review_w2v = create_w2v(X_test, 'cleaned_review')\n",
    "X_cv_review_w2v = create_w2v(X_cv, 'cleaned_review')\n",
    "\n",
    "# Concatenate the normalized data, sentiment scores, condition, year, and word2vec features for each dataset\n",
    "X_tr_6 = np.concatenate((X_train_num_2, X_train_sent_score, X_train_condition, X_train_year, X_train_review_w2v), axis=1)\n",
    "X_te_6 = np.concatenate((X_test_num_2, X_test_sent_score, X_test_condition, X_test_year, X_test_review_w2v), axis=1)\n",
    "X_cv_6 = np.concatenate((X_cv_num_2, X_cv_sent_score, X_cv_condition, X_cv_year, X_cv_review_w2v), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ed1a80-7cf8-4db8-b4c8-e1e929b93ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation set for early stopping\n",
    "eval_set = [(X_tr_6, y_train), (X_cv_6, y_cv)]\n",
    "\n",
    "# Initialize the XGBoost classifier with specified hyperparameters\n",
    "x_cfl_6 = XGBClassifier(n_estimators=3000, subsample=1, max_depth=7, learning_rate=0.05, colsample_bytree=1, nthread=-1, objective='binary:logistic', random_state=0)\n",
    "\n",
    "# Fit the classifier on the training data and use early stopping based on logloss on the evaluation set\n",
    "x_cfl_6.fit(X_tr_6, y_train, eval_set=eval_set, eval_metric='logloss', verbose=0, early_stopping_rounds=30)\n",
    "\n",
    "# Initialize a calibrated classifier on the XGBoost classifier with sigmoid method\n",
    "x_sig_clf_6 = CalibratedClassifierCV(x_cfl_6, method=\"sigmoid\")\n",
    "\n",
    "# Fit the calibrated classifier on the training data\n",
    "x_sig_clf_6.fit(X_tr_6, y_train)\n",
    "\n",
    "# Call the function to calculate and display the model metrics\n",
    "model_metrics(x_sig_clf_6, X_tr_6, X_te_6, X_cv_6)\n",
    "\n",
    "# Save the trained model for future use\n",
    "joblib.dump(x_sig_clf_6, '../BDS_project/W2V Model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c42325a-a9b7-444e-a7c8-ee1da4db7c52",
   "metadata": {},
   "source": [
    "## Ensemble learning approach for recommendation system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776f7572-b238-4def-b4b4-524c8c779b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the models\n",
    "model_1 = joblib.load('../BDS_project/Bow_model.pkl')\n",
    "model_2 = joblib.load('../BDS_project/tfidf_model.pkl')\n",
    "model_3 = joblib.load('../BDS_project/ngram_bow_model.pkl')\n",
    "model_4 = joblib.load('../BDS_project/ngram_tfidf_model.pkl')\n",
    "model_5 = joblib.load('../BDS_project/W2V Model.pkl')\n",
    "\n",
    "# Predicting using the models and adding the predictions as new columns in the test data\n",
    "X_test['model1'] = model_1.predict(X_te_1)\n",
    "X_test['model2'] = model_2.predict(X_te_2)\n",
    "X_test['model3'] = model_3.predict(X_te_3)\n",
    "X_test['model4'] = model_4.predict(X_te_4)\n",
    "X_test['model5'] = model_5.predict(X_te_5)\n",
    "\n",
    "def adjust_column(data, feature):\n",
    "    max_value = data[feature].max()\n",
    "    min_value = data[feature].min()\n",
    "    data[feature] = (data[feature] - min_value) / (max_value - min_value)\n",
    "    return data\n",
    "\n",
    "# Normalizing the 'usefulCount' feature\n",
    "X_test =  adjust_column(X_test, 'usefulCount')\n",
    "\n",
    "# Calculating a recommendation score based on the model predictions and the 'usefulCount' feature\n",
    "X_test['rec_score'] = (X_test['model1'] + X_test['model2'] + X_test['model3'] + X_test['model4'] + X_test['model5']) * X_test['usefulCount']\n",
    "\n",
    "# Saving the test data with the new columns to a csv file\n",
    "X_test.to_csv('../BDS_project/validation_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
