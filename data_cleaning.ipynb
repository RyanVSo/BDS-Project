{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "557008f2-bcd8-4ea9-9ae7-69e1b6799606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import regex as re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "import string\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18a77420-33cd-4d50-acad-e55c19d10ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../BDS_project'\n",
    "train_path = path + '/drug_train.tsv'\n",
    "test_path = path + '/drug_test.tsv'\n",
    "\n",
    "data_train = pd.read_csv(train_path, delimiter = '\\t')\n",
    "data_test = pd.read_csv(test_path, delimiter = '\\t')\n",
    "\n",
    "data_train.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "data_test.drop(columns = ['Unnamed: 0'], inplace = True)\n",
    "\n",
    "data = pd.concat([data_train,data_test])\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "\n",
    "data['review_sentiment'] = data['rating'].apply(lambda x: 1 if x > 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3dad5f-c169-4e35-aa3c-fa127255ab2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From Perfume Recommendation and Health Recommendation notebooks\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = text.replace('\\n',' ')\n",
    "    text = text.replace('\\r',' ')\n",
    "    text = text.replace('\\t',' ')\n",
    "    text = text.replace('-',' ')\n",
    "    text = text.replace(\"/\",' ')\n",
    "    text = text.replace(\">\",' ')\n",
    "    text = text.replace('\"',' ')\n",
    "    text = text.replace('?',' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb95c2-1cb4-48f5-a57e-0a81203ea20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "stop_words.remove('no')\n",
    "\n",
    "def nlp_preprocessing(review):\n",
    "    if type(review) is not int:\n",
    "        string = \"\"\n",
    "        review = preprocess_text(review)\n",
    "        review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "\n",
    "        review = re.sub('\\s+',' ', review)\n",
    "\n",
    "        review = review.lower()\n",
    "\n",
    "        for word in review.split():\n",
    "\n",
    "            if not word in stop_words:\n",
    "                word = stemmer.stem(word)\n",
    "                string += word + \" \"\n",
    "\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f02dab-b6f4-4a59-bb03-1a24c4699c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned_review'] = data['review'].apply(nlp_preprocessing)\n",
    "data['drugName'] = data['drugName'].apply(lambda x:x.lower()) \n",
    "data['condition'] = data['condition'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7a3ac2-c4b0-45f6-ba68-83932ed5f86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "data['sentiment_score'] = [sid.polarity_scores(v)['compound'] for v in data['review']]\n",
    "data['sentiment_score_clean'] = [sid.polarity_scores(v)['compound'] for v in data['cleaned_review']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847fb7fd-23ca-49e5-9b0a-f69c1858d27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(axis=0)\n",
    "data.reset_index(inplace=True,drop=True)\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = data['date'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9982ab03-4cb5-4188-a8aa-6ded94a9bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "data['word_count']=data[\"cleaned_review\"].apply(lambda x: len(str(x).split()))\n",
    "data['unique_word_count']=data[\"cleaned_review\"].apply(lambda x: len(set(str(x).split())))\n",
    "data['char_length']=data[\"cleaned_review\"].apply(lambda x: len(str(x)))\n",
    "data[\"count_punctuations\"] = data[\"review\"].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "data[\"stopword_count\"] = data[\"review\"].apply(lambda x: len([w for w in str(x).lower().split() if w in stop_words]))\n",
    "data[\"mean_word_len\"] = data[\"cleaned_review\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c48b8d-a7fb-4a99-96c6-1d3c9886777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def subj_obj_count(review):\n",
    "\n",
    "    sent = review\n",
    "    doc=nlp(sent)\n",
    "    sub_words = set([str(word) for word in doc if (word.dep_ == \"nsubj\")])\n",
    "\n",
    "    obj_words = set([str(word) for word in doc if (word.dep_ == \"dobj\")])\n",
    "\n",
    "    return len(sub_words),len(obj_words)\n",
    "\n",
    "from tqdm import tqdm\n",
    "count = []\n",
    "\n",
    "for r in tqdm(data['review']):\n",
    "    count.append(subj_obj_count(r))\n",
    "\n",
    "sub_obj = pd.DataFrame(count,columns=['subj_count','obj_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973e3419-5ddc-4753-90c8-d7031518738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_lst = nlp.pipe_labels['ner']\n",
    "\n",
    "def ner(review):\n",
    "    sent = review\n",
    "    doc=nlp(sent)\n",
    "    dic = {}.fromkeys(ner_lst,0)\n",
    "    for word in doc.ents:\n",
    "        dic[word.label_]+=1\n",
    "\n",
    "    return dic\n",
    "entity = pd.DataFrame([ner(r) for r in tqdm(data['cleaned_review'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc298842-a02f-4e9b-a226-20aac1cd475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "corpus = data['cleaned_review']\n",
    "\n",
    "lst_corpus = []\n",
    "for string in tqdm(corpus):\n",
    "    lst_words = string.split()\n",
    "    lst_grams = [\" \".join(lst_words[i:i + 1]) for i in range(0, len(lst_words), 1)]\n",
    "    lst_corpus.append(lst_grams)\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(lst_corpus)\n",
    "dic_corpus = [id2word.doc2bow(word) for word in lst_corpus]\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=dic_corpus, id2word=id2word, num_topics=20, chunksize=100, passes=10, alpha='auto', per_word_topics=True)\n",
    "\n",
    "train_vecs = []\n",
    "for i in range(len(corpus)):\n",
    "    top_topics = (\n",
    "        lda_model.get_document_topics(dic_corpus[i],\n",
    "                                      minimum_probability=0.0)\n",
    "    )\n",
    "    topic_vec = [top_topics[i][1] for i in range(20)]\n",
    "\n",
    "    train_vecs.append(topic_vec)\n",
    "topics = pd.DataFrame(train_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65214c4-6d15-4be9-a96d-4a274638b908",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([data,sub_obj,entity,topics],axis=1)\n",
    "data.to_csv('final_new_data_processed.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
